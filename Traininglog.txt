(carnd-term1) C:\jakub\pythonWorkspace\behavioralCloning>python trainTheModel.py --dataPath ./Record,./Recovery,./RecordOpposite,./Record1,./Recovery1,./Record1Opposite --epochs 25 --offset 0.15 --output LaNet1All25ep0_15
Using TensorFlow backend.
convert!!!!
./Record
./Recovery
./RecordOpposite
./Record1
./Recovery1
./Record1Opposite
Train on 22764 samples, validate on 5691 samples
Epoch 1/25
22764/22764 [==============================] - 1028s - loss: 0.1108 - val_loss: 0.0906
Epoch 2/25
22764/22764 [==============================] - 673s - loss: 0.0848 - val_loss: 0.0783
Epoch 3/25
22764/22764 [==============================] - 672s - loss: 0.0673 - val_loss: 0.0778
Epoch 4/25
22764/22764 [==============================] - 670s - loss: 0.0528 - val_loss: 0.0731
Epoch 5/25
22764/22764 [==============================] - 680s - loss: 0.0411 - val_loss: 0.0647
Epoch 6/25
22764/22764 [==============================] - 675s - loss: 0.0315 - val_loss: 0.0637
Epoch 7/25
22764/22764 [==============================] - 670s - loss: 0.0257 - val_loss: 0.0607
Epoch 8/25
22764/22764 [==============================] - 675s - loss: 0.0213 - val_loss: 0.0638
Epoch 9/25
22764/22764 [==============================] - 736s - loss: 0.0179 - val_loss: 0.0635
Epoch 10/25
22764/22764 [==============================] - 770s - loss: 0.0161 - val_loss: 0.0635
Epoch 11/25
22764/22764 [==============================] - 707s - loss: 0.0151 - val_loss: 0.0624
Epoch 12/25
22764/22764 [==============================] - 679s - loss: 0.0132 - val_loss: 0.0607
Epoch 13/25
22764/22764 [==============================] - 686s - loss: 0.0116 - val_loss: 0.0629
Epoch 14/25
22764/22764 [==============================] - 672s - loss: 0.0107 - val_loss: 0.0632
Epoch 15/25
22764/22764 [==============================] - 741s - loss: 0.0104 - val_loss: 0.0624
Epoch 16/25
22764/22764 [==============================] - 792s - loss: 0.0102 - val_loss: 0.0621
Epoch 17/25
22764/22764 [==============================] - 701s - loss: 0.0100 - val_loss: 0.0638
Epoch 18/25
22764/22764 [==============================] - 670s - loss: 0.0084 - val_loss: 0.0630
Epoch 19/25
22764/22764 [==============================] - 670s - loss: 0.0078 - val_loss: 0.0630
Epoch 20/25
22764/22764 [==============================] - 743s - loss: 0.0080 - val_loss: 0.0616
Epoch 21/25
22764/22764 [==============================] - 752s - loss: 0.0076 - val_loss: 0.0628
Epoch 22/25
22764/22764 [==============================] - 696s - loss: 0.0071 - val_loss: 0.0625
Epoch 23/25
22764/22764 [==============================] - 676s - loss: 0.0068 - val_loss: 0.0616
Epoch 24/25
22764/22764 [==============================] - 672s - loss: 0.0067 - val_loss: 0.0613
Epoch 25/25
22764/22764 [==============================] - 760s - loss: 0.0064 - val_loss: 0.0624

(carnd-term1) C:\jakub\pythonWorkspace\behavioralCloning>python trainNvidia.py --dataPath ./Record,./Recovery,./RecordOpposite,./Record1,./Recovery1,./Record1Opposite --epochs 25 --offset 0.15 --output nvidAll25ep0_15
Using TensorFlow backend.
convert!!!!
./Record
./Recovery
./RecordOpposite
./Record1
./Recovery1
./Record1Opposite
Train on 22764 samples, validate on 5691 samples
Epoch 1/25
22764/22764 [==============================] - 546s - loss: 0.1104 - val_loss: 0.1040
Epoch 2/25
22764/22764 [==============================] - 248s - loss: 0.0894 - val_loss: 0.0901
Epoch 3/25
22764/22764 [==============================] - 249s - loss: 0.0780 - val_loss: 0.0774
Epoch 4/25
22764/22764 [==============================] - 250s - loss: 0.0673 - val_loss: 0.0675
Epoch 5/25
22764/22764 [==============================] - 248s - loss: 0.0581 - val_loss: 0.0649
Epoch 6/25
22764/22764 [==============================] - 248s - loss: 0.0503 - val_loss: 0.0587
Epoch 7/25
22764/22764 [==============================] - 249s - loss: 0.0452 - val_loss: 0.0555
Epoch 8/25
22764/22764 [==============================] - 249s - loss: 0.0385 - val_loss: 0.0546
Epoch 9/25
22764/22764 [==============================] - 250s - loss: 0.0347 - val_loss: 0.0544
Epoch 10/25
22764/22764 [==============================] - 248s - loss: 0.0302 - val_loss: 0.0517
Epoch 11/25
22764/22764 [==============================] - 249s - loss: 0.0262 - val_loss: 0.0501
Epoch 12/25
22764/22764 [==============================] - 248s - loss: 0.0229 - val_loss: 0.0479
Epoch 13/25
22764/22764 [==============================] - 248s - loss: 0.0211 - val_loss: 0.0505
Epoch 14/25
22764/22764 [==============================] - 249s - loss: 0.0183 - val_loss: 0.0506
Epoch 15/25
22764/22764 [==============================] - 248s - loss: 0.0174 - val_loss: 0.0483
Epoch 16/25
22764/22764 [==============================] - 248s - loss: 0.0153 - val_loss: 0.0478
Epoch 17/25
22764/22764 [==============================] - 251s - loss: 0.0143 - val_loss: 0.0444
Epoch 18/25
22764/22764 [==============================] - 248s - loss: 0.0163 - val_loss: 0.0465
Epoch 19/25
22764/22764 [==============================] - 250s - loss: 0.0128 - val_loss: 0.0456
Epoch 20/25
22764/22764 [==============================] - 251s - loss: 0.0111 - val_loss: 0.0453
Epoch 21/25
22764/22764 [==============================] - 250s - loss: 0.0105 - val_loss: 0.0450
Epoch 22/25
22764/22764 [==============================] - 250s - loss: 0.0122 - val_loss: 0.0507
Epoch 23/25
22764/22764 [==============================] - 250s - loss: 0.0115 - val_loss: 0.0470
Epoch 24/25
22764/22764 [==============================] - 251s - loss: 0.0095 - val_loss: 0.0464
Epoch 25/25
22764/22764 [==============================] - 249s - loss: 0.0086 - val_loss: 0.0444

(carnd-term1) C:\jakub\pythonWorkspace\behavioralCloning>python trainNvidiaDrop.py --dataPath ./Record,./Recovery,./RecordOpposite,./Record1,./Recovery1,./Record1Opposite --epochs 25 --offset 0.15 --output nvidAllDrp25ep0_15
Using TensorFlow backend.
convert!!!!
./Record
./Recovery
./RecordOpposite
./Record1
./Recovery1
./Record1Opposite
Train on 22764 samples, validate on 5691 samples
Epoch 1/25
22764/22764 [==============================] - 624s - loss: 0.1134 - val_loss: 0.1021
Epoch 2/25
22764/22764 [==============================] - 327s - loss: 0.0935 - val_loss: 0.0847
Epoch 3/25
22764/22764 [==============================] - 327s - loss: 0.0848 - val_loss: 0.0779
Epoch 4/25
22764/22764 [==============================] - 326s - loss: 0.0756 - val_loss: 0.0739
Epoch 5/25
22764/22764 [==============================] - 325s - loss: 0.0677 - val_loss: 0.0655
Epoch 6/25
22764/22764 [==============================] - 325s - loss: 0.0606 - val_loss: 0.0630
Epoch 7/25
22764/22764 [==============================] - 325s - loss: 0.0569 - val_loss: 0.0605
Epoch 8/25
22764/22764 [==============================] - 325s - loss: 0.0512 - val_loss: 0.0568
Epoch 9/25
22764/22764 [==============================] - 328s - loss: 0.0466 - val_loss: 0.0545
Epoch 10/25
22764/22764 [==============================] - 327s - loss: 0.0436 - val_loss: 0.0495
Epoch 11/25
22764/22764 [==============================] - 327s - loss: 0.0414 - val_loss: 0.0481
Epoch 12/25
22764/22764 [==============================] - 326s - loss: 0.0385 - val_loss: 0.0500
Epoch 13/25
22764/22764 [==============================] - 325s - loss: 0.0365 - val_loss: 0.0467
Epoch 14/25
22764/22764 [==============================] - 327s - loss: 0.0338 - val_loss: 0.0474
Epoch 15/25
22764/22764 [==============================] - 326s - loss: 0.0329 - val_loss: 0.0471
Epoch 16/25
22764/22764 [==============================] - 326s - loss: 0.0308 - val_loss: 0.0431
Epoch 17/25
22764/22764 [==============================] - 328s - loss: 0.0291 - val_loss: 0.0460
Epoch 18/25
22764/22764 [==============================] - 326s - loss: 0.0280 - val_loss: 0.0436
Epoch 19/25
22764/22764 [==============================] - 326s - loss: 0.0263 - val_loss: 0.0430
Epoch 20/25
22764/22764 [==============================] - 326s - loss: 0.0260 - val_loss: 0.0439
Epoch 21/25
22764/22764 [==============================] - 325s - loss: 0.0254 - val_loss: 0.0429
Epoch 22/25
22764/22764 [==============================] - 326s - loss: 0.0242 - val_loss: 0.0417
Epoch 23/25
22764/22764 [==============================] - 326s - loss: 0.0229 - val_loss: 0.0408
Epoch 24/25
22764/22764 [==============================] - 326s - loss: 0.0227 - val_loss: 0.0437
Epoch 25/25
22764/22764 [==============================] - 326s - loss: 0.0231 - val_loss: 0.0415
